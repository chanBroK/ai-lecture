{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class to use data_loader(batch_size)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = torch.Tensor(x).to(device)\n",
    "        self.y = torch.LongTensor(y).to(device)\n",
    "        self.len = len(x)\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "train = pd.read_csv(\"./data1/mnist_train.csv\")\n",
    "test = pd.read_csv(\"./data1/mnist_test.csv\")\n",
    "submission = pd.read_csv(\"./data1/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    0    1    2    3    4    5    6    7    8  ...  775  776  \\\n",
      "0           0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "1           1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "2           2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "3           3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "4           4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "\n",
      "   777  778  779  780  781  782  783  784  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  5.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  4.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  9.0  \n",
      "\n",
      "[5 rows x 786 columns]\n",
      "   Unnamed: 0    0    1    2    3    4    5    6    7    8  ...  774  775  \\\n",
      "0           0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "1           1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "2           2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "3           3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "4           4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "\n",
      "   776  777  778  779  780  781  782  783  \n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "   id  Label\n",
      "0   0      1\n",
      "1   1      1\n",
      "2   2      1\n",
      "3   3      1\n",
      "4   4      1\n"
     ]
    }
   ],
   "source": [
    "# check data\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing and set x_train, y_train ,x_test\n",
    "x_train = np.array(train.drop(['0','784'],axis=1))\n",
    "y_train = np.array(train['784']).astype(int)\n",
    "x_test = np.array(test.drop(['0'],axis=1))\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# x_train = torch.Tensor(x_train).to(device)\n",
    "# y_train = torch.LongTensor(y_train).to(device)\n",
    "x_test = torch.Tensor(x_test).to(device)\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "train_dataset= TrainDataset(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set layer\n",
    "layer1 = torch.nn.Linear(784,512,bias=True).to(device)\n",
    "layer2 = torch.nn.Linear(512,512,bias=True).to(device)\n",
    "layer3 = torch.nn.Linear(512,512,bias=True).to(device)\n",
    "layer4 = torch.nn.Linear(512,512,bias=True).to(device)\n",
    "layer5 = torch.nn.Linear(512,512,bias=True).to(device)\n",
    "layer6 = torch.nn.Linear(512,512,bias=True).to(device)\n",
    "layer7 = torch.nn.Linear(512,len(le.classes_),bias=True).to(device)\n",
    "\n",
    "relu = torch.nn.ReLU().to(device)\n",
    "dropout = torch.nn.Dropout(0.3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1136, -0.0801, -0.0845,  ...,  0.0988,  0.0007, -0.0483],\n",
       "        [ 0.0135, -0.0115,  0.0006,  ..., -0.1359,  0.0526,  0.0597],\n",
       "        [ 0.0713,  0.0166, -0.0178,  ..., -0.0105, -0.0389,  0.0635],\n",
       "        ...,\n",
       "        [-0.0429, -0.0455,  0.0215,  ...,  0.0077, -0.0574, -0.0597],\n",
       "        [ 0.0161,  0.0767, -0.1157,  ..., -0.0640, -0.0232, -0.0452],\n",
       "        [ 0.0147,  0.0385,  0.0480,  ...,  0.0763,  0.0079, -0.0304]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init layers\n",
    "torch.nn.init.xavier_normal_(layer1.weight)\n",
    "torch.nn.init.xavier_normal_(layer2.weight)\n",
    "torch.nn.init.xavier_normal_(layer3.weight)\n",
    "torch.nn.init.xavier_normal_(layer4.weight)\n",
    "torch.nn.init.xavier_normal_(layer5.weight)\n",
    "torch.nn.init.xavier_normal_(layer6.weight)\n",
    "torch.nn.init.xavier_normal_(layer7.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model\n",
    "model = torch.nn.Sequential(layer1,relu,dropout,\n",
    "                            layer2,relu,dropout,\n",
    "                            layer3,relu,dropout,\n",
    "                            layer4,relu,dropout,\n",
    "                            layer5,relu,dropout,\n",
    "                            layer6,relu,dropout,\n",
    "                            layer7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Learning params and function\n",
    "# 목표 !! cost 0.04 정도 \n",
    "# layer 5\n",
    "# epochs 15 , batch_size 100 -> cost 0.4/// 0.9566\n",
    "# epochs 50, batch_size 600 -> cost 0.17// 0.9706\n",
    "# epochs 100, batch_size 600 -> cost 0.073// -> overfitting? 0.9796\n",
    "# epochs 100, batch_size 1000 -> cost 0.094 // 0.9785\n",
    "# epochs 100, batch_size 500 -> cost  0.067 // 0.9797\n",
    "# epochs 100, batch_size 300 -> cost  0.114676796 // 0.9782\n",
    "# 실습 7번 세팅대로 -> cost 0.24 // 0.96\n",
    "# layer 5->7, epochs 100, batch_size 500 ->  cost 0.08// 0.9657\n",
    "# scaling..... -> 0.03\n",
    "\n",
    "\n",
    "epochs = 150\n",
    "lr = 0.001\n",
    "batch_size = 500\n",
    "optim = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle = True,\n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 00 || cost = 0.782132208\n",
      "epoch = 01 || cost = 0.302209675\n",
      "epoch = 02 || cost = 0.238780037\n",
      "epoch = 03 || cost = 0.201770335\n",
      "epoch = 04 || cost = 0.178874761\n",
      "epoch = 05 || cost = 0.159889638\n",
      "epoch = 06 || cost = 0.143201441\n",
      "epoch = 07 || cost = 0.133738428\n",
      "epoch = 08 || cost = 0.122382760\n",
      "epoch = 09 || cost = 0.117153987\n",
      "epoch = 10 || cost = 0.105922088\n",
      "epoch = 11 || cost = 0.098460883\n",
      "epoch = 12 || cost = 0.102145098\n",
      "epoch = 13 || cost = 0.091569439\n",
      "epoch = 14 || cost = 0.089838959\n",
      "epoch = 15 || cost = 0.086246967\n",
      "epoch = 16 || cost = 0.085018948\n",
      "epoch = 17 || cost = 0.076771088\n",
      "epoch = 18 || cost = 0.074758701\n",
      "epoch = 19 || cost = 0.074379779\n",
      "epoch = 20 || cost = 0.071793377\n",
      "epoch = 21 || cost = 0.067144021\n",
      "epoch = 22 || cost = 0.063606210\n",
      "epoch = 23 || cost = 0.067003720\n",
      "epoch = 24 || cost = 0.063963287\n",
      "epoch = 25 || cost = 0.062828757\n",
      "epoch = 26 || cost = 0.060665675\n",
      "epoch = 27 || cost = 0.057294358\n",
      "epoch = 28 || cost = 0.055789798\n",
      "epoch = 29 || cost = 0.061102841\n",
      "epoch = 30 || cost = 0.056528438\n",
      "epoch = 31 || cost = 0.052390065\n",
      "epoch = 32 || cost = 0.051915132\n",
      "epoch = 33 || cost = 0.057301644\n",
      "epoch = 34 || cost = 0.051754083\n",
      "epoch = 35 || cost = 0.052152067\n",
      "epoch = 36 || cost = 0.050764475\n",
      "epoch = 37 || cost = 0.053689986\n",
      "epoch = 38 || cost = 0.051443812\n",
      "epoch = 39 || cost = 0.053172391\n",
      "epoch = 40 || cost = 0.045253307\n",
      "epoch = 41 || cost = 0.046518408\n",
      "epoch = 42 || cost = 0.045592282\n",
      "epoch = 43 || cost = 0.047303814\n",
      "epoch = 44 || cost = 0.045618419\n",
      "epoch = 45 || cost = 0.045356620\n",
      "epoch = 46 || cost = 0.043587957\n",
      "epoch = 47 || cost = 0.046043117\n",
      "epoch = 48 || cost = 0.041531861\n",
      "epoch = 49 || cost = 0.047736131\n",
      "epoch = 50 || cost = 0.044486273\n",
      "epoch = 51 || cost = 0.039941501\n",
      "epoch = 52 || cost = 0.040990561\n",
      "epoch = 53 || cost = 0.042658687\n",
      "epoch = 54 || cost = 0.041589744\n",
      "epoch = 55 || cost = 0.042589489\n",
      "epoch = 56 || cost = 0.038315088\n",
      "epoch = 57 || cost = 0.038613755\n",
      "epoch = 58 || cost = 0.039417218\n",
      "epoch = 59 || cost = 0.040702377\n",
      "epoch = 60 || cost = 0.044556234\n",
      "epoch = 61 || cost = 0.039585657\n",
      "epoch = 62 || cost = 0.037266742\n",
      "epoch = 63 || cost = 0.035433013\n",
      "epoch = 64 || cost = 0.036892448\n",
      "epoch = 65 || cost = 0.036045428\n",
      "epoch = 66 || cost = 0.041876011\n",
      "epoch = 67 || cost = 0.039921574\n",
      "epoch = 68 || cost = 0.039264929\n",
      "epoch = 69 || cost = 0.039250810\n",
      "epoch = 70 || cost = 0.039225709\n",
      "epoch = 71 || cost = 0.034064557\n",
      "epoch = 72 || cost = 0.031084850\n",
      "epoch = 73 || cost = 0.039947297\n",
      "epoch = 74 || cost = 0.038188599\n",
      "epoch = 75 || cost = 0.035559345\n",
      "epoch = 76 || cost = 0.038736772\n",
      "epoch = 77 || cost = 0.040676132\n",
      "epoch = 78 || cost = 0.037454549\n",
      "epoch = 79 || cost = 0.033608854\n",
      "epoch = 80 || cost = 0.034006890\n",
      "epoch = 81 || cost = 0.032399669\n",
      "epoch = 82 || cost = 0.034453165\n",
      "epoch = 83 || cost = 0.035623383\n",
      "epoch = 84 || cost = 0.038614269\n",
      "epoch = 85 || cost = 0.035717618\n",
      "epoch = 86 || cost = 0.034908447\n",
      "epoch = 87 || cost = 0.034203023\n",
      "epoch = 88 || cost = 0.035399131\n",
      "epoch = 89 || cost = 0.036508229\n",
      "epoch = 90 || cost = 0.035191357\n",
      "epoch = 91 || cost = 0.032951739\n",
      "epoch = 92 || cost = 0.033990003\n",
      "epoch = 93 || cost = 0.032581005\n",
      "epoch = 94 || cost = 0.032043185\n",
      "epoch = 95 || cost = 0.036634367\n",
      "epoch = 96 || cost = 0.033815738\n",
      "epoch = 97 || cost = 0.033408687\n",
      "epoch = 98 || cost = 0.033666082\n",
      "epoch = 99 || cost = 0.033234354\n",
      "epoch = 100 || cost = 0.035532348\n",
      "epoch = 101 || cost = 0.035292119\n",
      "epoch = 102 || cost = 0.031701814\n",
      "epoch = 103 || cost = 0.030137416\n",
      "epoch = 104 || cost = 0.030072613\n",
      "epoch = 105 || cost = 0.030364314\n",
      "epoch = 106 || cost = 0.029946275\n",
      "epoch = 107 || cost = 0.035794701\n",
      "epoch = 108 || cost = 0.033937555\n",
      "epoch = 109 || cost = 0.031720366\n",
      "epoch = 110 || cost = 0.033142287\n",
      "epoch = 111 || cost = 0.032384485\n",
      "epoch = 112 || cost = 0.031072179\n",
      "epoch = 113 || cost = 0.027446041\n",
      "epoch = 114 || cost = 0.028598892\n",
      "epoch = 115 || cost = 0.035037071\n",
      "epoch = 116 || cost = 0.036842640\n",
      "epoch = 117 || cost = 0.033640381\n",
      "epoch = 118 || cost = 0.034497600\n",
      "epoch = 119 || cost = 0.031959329\n",
      "epoch = 120 || cost = 0.030542493\n",
      "epoch = 121 || cost = 0.031143945\n",
      "epoch = 122 || cost = 0.030208457\n",
      "epoch = 123 || cost = 0.033252478\n",
      "epoch = 124 || cost = 0.029764436\n",
      "epoch = 125 || cost = 0.029951723\n",
      "epoch = 126 || cost = 0.030941973\n",
      "epoch = 127 || cost = 0.030359274\n",
      "epoch = 128 || cost = 0.030561917\n",
      "epoch = 129 || cost = 0.028833443\n",
      "epoch = 130 || cost = 0.031332418\n",
      "epoch = 131 || cost = 0.029191926\n",
      "epoch = 132 || cost = 0.033043377\n",
      "epoch = 133 || cost = 0.033793490\n",
      "epoch = 134 || cost = 0.028075742\n",
      "epoch = 135 || cost = 0.026918447\n",
      "epoch = 136 || cost = 0.029531520\n",
      "epoch = 137 || cost = 0.031993378\n",
      "epoch = 138 || cost = 0.029063750\n",
      "epoch = 139 || cost = 0.026465792\n",
      "epoch = 140 || cost = 0.029185893\n",
      "epoch = 141 || cost = 0.030550970\n",
      "epoch = 142 || cost = 0.029488860\n",
      "epoch = 143 || cost = 0.032429095\n",
      "epoch = 144 || cost = 0.031893134\n",
      "epoch = 145 || cost = 0.034685802\n",
      "epoch = 146 || cost = 0.031449456\n",
      "epoch = 147 || cost = 0.028375953\n",
      "epoch = 148 || cost = 0.026430057\n",
      "epoch = 149 || cost = 0.028162356\n",
      "epoch = 150 || cost = 0.027678339\n"
     ]
    }
   ],
   "source": [
    "# learning\n",
    "model.train()\n",
    "batch_num = len(data_loader)\n",
    "for epoch in range(epochs+1):\n",
    "    avg_cost = 0\n",
    "    for x,y in data_loader:\n",
    "        optim.zero_grad()\n",
    "        h = model(x)\n",
    "        cost = loss(h,y)\n",
    "        cost.backward()\n",
    "        optim.step()\n",
    "        avg_cost += cost\n",
    "\n",
    "    print(\"epoch = {:02d} || cost = {:.9f}\".format(epoch,avg_cost/batch_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.999817\n"
     ]
    }
   ],
   "source": [
    "# accuracy \n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predict = model(train_dataset.x)\n",
    "predict = torch.softmax(predict,dim = 1)\n",
    "predict = torch.argmax(predict,dim = 1)\n",
    "# print(predict)\n",
    "# print(dataset.y)\n",
    "accuracy = (predict == train_dataset.y).float().mean()\n",
    "print(\"Accuracy = {:f}\".format(accuracy.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  Label\n",
      "0        0      7\n",
      "1        1      2\n",
      "2        2      1\n",
      "3        3      0\n",
      "4        4      4\n",
      "...    ...    ...\n",
      "9995  9995      2\n",
      "9996  9996      3\n",
      "9997  9997      4\n",
      "9998  9998      5\n",
      "9999  9999      6\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# submission\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predict = model(x_test)\n",
    "predict = torch.softmax(predict,dim=1)\n",
    "predict = torch.argmax(predict,dim=1)\n",
    "predict = le.inverse_transform(predict.cpu().detach().numpy())\n",
    "submission['Label'] = predict\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\",index=False)\n",
    "# 왜 학습 이후에 계속 한가지 정수로만 결과가 나올까?(batch_size 적용 X)\n",
    "# batch size가 없으면 한가지 정수로만 예측 결과가 나온다. -> batch size 적용이 필요 \n",
    "# batch size를 반복문으로 구현하여도 제대로된 예측 결과가 나오지 않음\n",
    "# Dataloader를 통해 batch size를 사용해야함 \n",
    "# Data loader 사용을 위해 Dataset Class 의 인스턴스가 필요함 \n",
    "# CSV 파일에서 읽어온 데이터를 Dataset Class로 만들기 위해 Dataset을 상속받은 클래스 생성\n",
    "# 생성자, __len__, __getitem__ 함수 구현해야함 \n",
    "\n",
    "\n",
    "### 위의 추론은 틀렸다. \n",
    "# batch_size = 600 기준 , 신경망의 개수가 5개보다 많아지면 Gradient Vanishing이 발생하는 것 같다.\n",
    "# 6개 부터는 위와 같이 한가지 정수로만 예측결과를 얻게 된다.\n",
    "# 우선!!제대로된 batch_size의 적용을 위해 Dataset 클래스를 상속하는 커스텀 클래스를 생성해야함!!\n",
    "# + learning late 조정 와 batch_size 조정 \n",
    "# ....실습의 데이터 처럼 스케일링 되어서 0~1 사이의 값인 줄 알았는데...아니였다. 스케일링 문제였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
