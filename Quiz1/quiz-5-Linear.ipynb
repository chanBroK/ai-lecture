{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "/kaggle/input/2021-ai-quiz1-p5/train.csv\n",
    "/kaggle/input/2021-ai-quiz1-p5/test.csv\n",
    "/kaggle/input/2021-ai-quiz1-p5/submit_sample.csv\n",
    "import torch # 인공지능 학습을 위한 pytorch 불러오기\n",
    "import torch.nn as nn # torch에서 제공하는 nn 모듈 불러오기\n",
    "import torch.optim as optim # 학습시 사용할 optimizer가 담긴 torch.optim 불러오기\n",
    "\n",
    "device = torch.device(\"cuda\") #gpu 사용을 위해 cuda 할당\n",
    "torch.manual_seed(1) # seed 1로 고정 (같은 input, 같은 output을 위해)\n",
    "<torch._C.Generator at 0x7fd8f8ce83b0>\n",
    "train = pd.read_csv('../input/2021-ai-quiz1-p5/train.csv') # train 데이터 불러오기 (label 포함)\n",
    "test = pd.read_csv('../input/2021-ai-quiz1-p5/test.csv') #test 데이터 불러오기\n",
    "submit = pd.read_csv('../input/2021-ai-quiz1-p5/submit_sample.csv') #제출 파일 형식 불러오기\n",
    "train_x = train.drop(['diagnosis'],axis=1)\n",
    "train_y = train['diagnosis']\n",
    "train_x = torch.FloatTensor(np.array(train_x)).to(device) # 저장했던 train_x  numpy 배열 -> tensor로 전환하고 gpu 사용을 위해 설정\n",
    "train_y = torch.FloatTensor(np.array(train_y)).to(device) # 저장했던 train_y  numpy 배열 -> tensor로 전환하고 gpu 사용을 위해 설정\n",
    "test = torch.FloatTensor(np.array(test)).to(device) # 저장했던 test  numpy 배열 -> tensor로 전환하고 gpu 사용을 위해 설정\n",
    "print(train_x.shape) # 각 데이터 shape 확인\n",
    "print(train_y.shape)\n",
    "torch.Size([398, 30])\n",
    "torch.Size([398])\n",
    "# layor 정의 , sequential -> 순서대로 layor 통과\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(30,15,bias=True).to(device), # input 8 , output 16  형태로 linear layor\n",
    "    nn.Linear(15,15,bias=True).to(device), # input 16 , output 16  형태로 linear layor\n",
    "    nn.ReLU().to(device),\n",
    "    nn.Linear(15,1,bias=True).to(device), # input 16 , output 1  형태로 linear layor (이진 분류 문제이므로)\n",
    ")\n",
    "loss = torch.nn.BCEWithLogitsLoss() # logstic이 포함된 loss function\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.0001) # optimizer로 adam 사용\n",
    "train_y = train_y.reshape(-1,1)\n",
    "epochs = 1000 # epoch 설정\n",
    "\n",
    "for epoch in range(epochs): #각 에포크 마다\n",
    "    hx = model(train_x)  # 정의내린 model에 train_x 데이터 입력으로 넣어 가설 설정\n",
    "    cost = loss(hx,train_y) # 예측값과 true값의 오차 계산 ( BCEWithLogitsLoss사용)\n",
    "    \n",
    "    optimizer.zero_grad() # 파라미터 초기화\n",
    "    cost.backward() # 오차 전달\n",
    "    optimizer.step() # 업데이트\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : {}/{}  Cost : {}\".format(epoch,epochs,cost.item())) # 설정된 에포크 마다 학습 결과(COST) 출력\n",
    "Epoch : 0/1000  Cost : 6.601346015930176\n",
    "Epoch : 10/1000  Cost : 0.47416484355926514\n",
    "Epoch : 20/1000  Cost : 0.4523750841617584\n",
    "Epoch : 30/1000  Cost : 0.44543230533599854\n",
    "Epoch : 40/1000  Cost : 0.44007763266563416\n",
    "Epoch : 50/1000  Cost : 0.4353771209716797\n",
    "Epoch : 60/1000  Cost : 0.43103086948394775\n",
    "Epoch : 70/1000  Cost : 0.4269461929798126\n",
    "Epoch : 80/1000  Cost : 0.4230990707874298\n",
    "Epoch : 90/1000  Cost : 0.41941162943840027\n",
    "Epoch : 100/1000  Cost : 0.4158487021923065\n",
    "Epoch : 110/1000  Cost : 0.4124198257923126\n",
    "Epoch : 120/1000  Cost : 0.4091043770313263\n",
    "Epoch : 130/1000  Cost : 0.40591445565223694\n",
    "Epoch : 140/1000  Cost : 0.40282759070396423\n",
    "Epoch : 150/1000  Cost : 0.399844765663147\n",
    "Epoch : 160/1000  Cost : 0.39694952964782715\n",
    "Epoch : 170/1000  Cost : 0.3941314220428467\n",
    "Epoch : 180/1000  Cost : 0.39138907194137573\n",
    "Epoch : 190/1000  Cost : 0.3887217938899994\n",
    "Epoch : 200/1000  Cost : 0.38612478971481323\n",
    "Epoch : 210/1000  Cost : 0.38357681035995483\n",
    "Epoch : 220/1000  Cost : 0.38104185461997986\n",
    "Epoch : 230/1000  Cost : 0.37860000133514404\n",
    "Epoch : 240/1000  Cost : 0.376216322183609\n",
    "Epoch : 250/1000  Cost : 0.3738771378993988\n",
    "Epoch : 260/1000  Cost : 0.371593177318573\n",
    "Epoch : 270/1000  Cost : 0.3693593740463257\n",
    "Epoch : 280/1000  Cost : 0.3671792447566986\n",
    "Epoch : 290/1000  Cost : 0.36503562331199646\n",
    "Epoch : 300/1000  Cost : 0.3629070520401001\n",
    "Epoch : 310/1000  Cost : 0.3608538806438446\n",
    "Epoch : 320/1000  Cost : 0.35883525013923645\n",
    "Epoch : 330/1000  Cost : 0.35686028003692627\n",
    "Epoch : 340/1000  Cost : 0.3549230992794037\n",
    "Epoch : 350/1000  Cost : 0.3530223071575165\n",
    "Epoch : 360/1000  Cost : 0.35115325450897217\n",
    "Epoch : 370/1000  Cost : 0.3493123948574066\n",
    "Epoch : 380/1000  Cost : 0.347505658864975\n",
    "Epoch : 390/1000  Cost : 0.3457322120666504\n",
    "Epoch : 400/1000  Cost : 0.3439944386482239\n",
    "Epoch : 410/1000  Cost : 0.34228065609931946\n",
    "Epoch : 420/1000  Cost : 0.3405929505825043\n",
    "Epoch : 430/1000  Cost : 0.33893635869026184\n",
    "Epoch : 440/1000  Cost : 0.33730655908584595\n",
    "Epoch : 450/1000  Cost : 0.33570218086242676\n",
    "Epoch : 460/1000  Cost : 0.33412492275238037\n",
    "Epoch : 470/1000  Cost : 0.3325739800930023\n",
    "Epoch : 480/1000  Cost : 0.3310481607913971\n",
    "Epoch : 490/1000  Cost : 0.32954663038253784\n",
    "Epoch : 500/1000  Cost : 0.3280685544013977\n",
    "Epoch : 510/1000  Cost : 0.32661399245262146\n",
    "Epoch : 520/1000  Cost : 0.3251829743385315\n",
    "Epoch : 530/1000  Cost : 0.3237740695476532\n",
    "Epoch : 540/1000  Cost : 0.3223881125450134\n",
    "Epoch : 550/1000  Cost : 0.32101938128471375\n",
    "Epoch : 560/1000  Cost : 0.3196375370025635\n",
    "Epoch : 570/1000  Cost : 0.3182770609855652\n",
    "Epoch : 580/1000  Cost : 0.3168753981590271\n",
    "Epoch : 590/1000  Cost : 0.31538093090057373\n",
    "Epoch : 600/1000  Cost : 0.31406405568122864\n",
    "Epoch : 610/1000  Cost : 0.3127565085887909\n",
    "Epoch : 620/1000  Cost : 0.31147992610931396\n",
    "Epoch : 630/1000  Cost : 0.3102267384529114\n",
    "Epoch : 640/1000  Cost : 0.30899518728256226\n",
    "Epoch : 650/1000  Cost : 0.3077833354473114\n",
    "Epoch : 660/1000  Cost : 0.30659258365631104\n",
    "Epoch : 670/1000  Cost : 0.3054220378398895\n",
    "Epoch : 680/1000  Cost : 0.3042723536491394\n",
    "Epoch : 690/1000  Cost : 0.30314338207244873\n",
    "Epoch : 700/1000  Cost : 0.3020332157611847\n",
    "Epoch : 710/1000  Cost : 0.3009422719478607\n",
    "Epoch : 720/1000  Cost : 0.2998700439929962\n",
    "Epoch : 730/1000  Cost : 0.2988174259662628\n",
    "Epoch : 740/1000  Cost : 0.2977842688560486\n",
    "Epoch : 750/1000  Cost : 0.2967700958251953\n",
    "Epoch : 760/1000  Cost : 0.29577481746673584\n",
    "Epoch : 770/1000  Cost : 0.2947975695133209\n",
    "Epoch : 780/1000  Cost : 0.2938377857208252\n",
    "Epoch : 790/1000  Cost : 0.29289510846138\n",
    "Epoch : 800/1000  Cost : 0.2919688820838928\n",
    "Epoch : 810/1000  Cost : 0.29105904698371887\n",
    "Epoch : 820/1000  Cost : 0.2901650071144104\n",
    "Epoch : 830/1000  Cost : 0.2892862856388092\n",
    "Epoch : 840/1000  Cost : 0.28842219710350037\n",
    "Epoch : 850/1000  Cost : 0.2875741124153137\n",
    "Epoch : 860/1000  Cost : 0.28674134612083435\n",
    "Epoch : 870/1000  Cost : 0.28592249751091003\n",
    "Epoch : 880/1000  Cost : 0.28511714935302734\n",
    "Epoch : 890/1000  Cost : 0.2843261957168579\n",
    "Epoch : 900/1000  Cost : 0.2835491895675659\n",
    "Epoch : 910/1000  Cost : 0.2827853560447693\n",
    "Epoch : 920/1000  Cost : 0.2820342183113098\n",
    "Epoch : 930/1000  Cost : 0.2812958359718323\n",
    "Epoch : 940/1000  Cost : 0.2805696129798889\n",
    "Epoch : 950/1000  Cost : 0.27985528111457825\n",
    "Epoch : 960/1000  Cost : 0.27915236353874207\n",
    "Epoch : 970/1000  Cost : 0.278460294008255\n",
    "Epoch : 980/1000  Cost : 0.27777907252311707\n",
    "Epoch : 990/1000  Cost : 0.277108758687973\n",
    "hx = model(test) # 학습이 완료된 model에 test 데이터 적용\n",
    "pred = (hx>=torch.FloatTensor([0.5]).to(device))\n",
    "pred = pred.reshape(-1).tolist()\n",
    "# true -> 1로 false -> 0으로 변환\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == True:\n",
    "        pred[i] = 1\n",
    "    elif pred[i] == False:\n",
    "        pred[i] = 0\n",
    "submit['diagnosis'] = pred #저장\n",
    "submit.to_csv('submit.csv',index=False) #파일 생성\n",
    "submit\n",
    "id\tdiagnosis\n",
    "0\t0\t0\n",
    "1\t1\t0\n",
    "2\t2\t0\n",
    "3\t3\t0\n",
    "4\t4\t0\n",
    "...\t...\t...\n",
    "166\t166\t0\n",
    "167\t167\t0\n",
    "168\t168\t0\n",
    "169\t169\t0\n",
    "170\t170\t0\n",
    "171 rows × 2 columns\n",
    "\n",
    "hx = model(train_x) # 학습이 완료된 model에 test 데이터 적용\n",
    "pred = (hx>=torch.FloatTensor([0.5]).to(device))\n",
    "# true -> 1로 false -> 0으로 변환\n",
    "for i in range(len(pred)):\n",
    "    if pred[i] == True:\n",
    "        pred[i] = 1\n",
    "    elif pred[i] == False:\n",
    "        pred[i] = 0\n",
    "train_y = train_y.reshape(-1).tolist()\n",
    "correct = 0\n",
    "for i in range(len(pred)):\n",
    "    if train_y[i] == pred[i]:\n",
    "        correct = correct+1\n",
    "print(correct)\n",
    "361\n",
    "accuracy = correct / len(pred)\n",
    "print(\"accuracy : {}\",  (accuracy*100))\n",
    "accuracy : {} 90.7035175879397\n",
    "Learning rate : 0.0001 Optimizer : Adam Epoch : 100 Layor : Linear 3개 층 , LeLU 1개 층 정확도 : 87.1859296482412\n",
    "\n",
    "Learning rate : 0.0001 Optimizer : Adam Epoch : 1000 Layor : Linear 3개 층 , LeLU 1개 층 정확도 : 90.7035175879397\n",
    "\n",
    "Learning rate : 0.1 Optimizer : Adam Epoch : 1000 Layor : Linear 3개 층 , LeLU 1개 층 정확도 : 63.31658291457286\n",
    "\n",
    "으로써 에포크를 1000으로 증가시키면 정확도가 올라가고 learning rate가 0.1에선 cost가 발산하며 정확도가 매우 떨어지는 것을 확인할 수 있었다 따라서 epoch 1000에 learning rate 0.0001이 가장 적합하다고 판단했다\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
