{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "/kaggle/input/2021-ai-quiz1-p3/train.csv\n",
    "/kaggle/input/2021-ai-quiz1-p3/test.csv\n",
    "/kaggle/input/2021-ai-quiz1-p3/submit_sample.csv\n",
    "# torch 관련 library 호출\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# GPU 사용을 위한 device 선언\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "'cpu'\n",
    "# 데이터 로드\n",
    "\n",
    "train = pd.read_csv(\"/kaggle/input/2021-ai-quiz1-p3/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/2021-ai-quiz1-p3/test.csv\")\n",
    "submit = pd.read_csv(\"/kaggle/input/2021-ai-quiz1-p3/submit_sample.csv\")\n",
    "train\n",
    "Unnamed: 0\t0\t1\t2\t3\t4\t5\t6\t7\t8\t...\t141\t142\t143\t144\t145\t146\t147\t148\t149\t0.1\n",
    "0\t0\t-2.075606\t-1.045790\t2.126936\t0.036825\t-0.757574\t-0.517365\t0.855506\t1.051939\t0.457736\t...\t0.766970\t-0.424478\t-0.124687\t-1.496749\t0.447682\t0.436117\t0.456781\t-0.871528\t2.808375\t3\n",
    "1\t1\t1.321112\t0.592836\t0.534154\t0.122660\t1.182957\t-0.673364\t-0.182102\t1.064393\t0.870060\t...\t0.374759\t-0.317582\t-0.199934\t-0.573055\t0.597456\t-0.123165\t1.199251\t-0.920927\t1.424777\t1\n",
    "2\t2\t-0.761193\t-0.019730\t-0.239907\t0.499094\t1.304381\t-0.561011\t0.069747\t1.620910\t0.118996\t...\t-1.785485\t0.708388\t0.001030\t-0.015270\t1.503353\t0.867290\t1.289758\t-0.995063\t-0.750737\t2\n",
    "3\t3\t-0.117408\t0.116545\t-0.009745\t2.104061\t-0.549831\t0.623429\t0.885584\t0.496275\t-0.346002\t...\t-1.107025\t-0.314110\t1.143012\t1.028699\t0.271098\t0.230415\t0.643734\t-2.632324\t-0.415003\t6\n",
    "4\t4\t-0.396370\t0.426845\t-0.250984\t-0.651813\t1.795055\t0.917808\t0.349123\t-1.391063\t1.175008\t...\t0.595545\t-0.752304\t-0.414062\t0.662435\t0.014245\t0.087689\t0.695441\t0.475167\t-0.314527\t4\n",
    "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
    "961\t961\t-0.087658\t0.502053\t-0.153144\t-1.402996\t-0.629632\t-0.907732\t0.724808\t-1.723367\t-1.083608\t...\t-0.089831\t0.983060\t0.218364\t0.235335\t-1.479324\t-2.567603\t-0.321805\t-1.250793\t-2.288816\t1\n",
    "962\t962\t0.481050\t-0.244974\t0.390068\t-2.094188\t-1.778837\t-1.525156\t-0.293080\t-0.374690\t1.558995\t...\t0.287247\t0.113489\t-1.096534\t1.646904\t1.081852\t0.553180\t1.039318\t1.006797\t1.336859\t6\n",
    "963\t963\t-0.760314\t0.040259\t-0.212453\t-1.249947\t0.036339\t1.906293\t-0.274605\t-0.407281\t-1.245358\t...\t1.460066\t0.468226\t0.619295\t0.146351\t-0.362881\t0.741775\t-0.072357\t-0.197916\t-0.548080\t3\n",
    "964\t964\t-0.267422\t0.838640\t-0.222198\t-0.952942\t0.468293\t0.900771\t-0.627356\t0.517218\t1.974085\t...\t2.055150\t1.007875\t-0.011870\t0.352795\t-0.213072\t1.968517\t-1.882339\t1.767359\t-1.468716\t2\n",
    "965\t965\t-1.319565\t-1.126696\t0.156169\t-1.307257\t-0.317330\t1.714553\t0.443122\t0.402669\t1.907508\t...\t-1.172470\t-1.295797\t-2.220556\t0.596601\t-2.465747\t1.625675\t1.070389\t-0.375185\t1.220137\t1\n",
    "966 rows × 152 columns\n",
    "\n",
    "# 학습 데이터셋 구축\n",
    "\n",
    "x = train.drop([\"Unnamed: 0\", \"0.1\"], axis=1)\n",
    "y = train[\"0.1\"]\n",
    "x_test = test.drop([\"Unnamed: 0\"], axis=1)\n",
    "# 크게 튀는 값은 없어 보이나, 데이터 자체의 안정화를 위해 가장 기본이 되는 전처리 사용\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)\n",
    "x_test = sc.transform(x_test)\n",
    "# 학습 데이터를 tensor화 시킨 후 GPU 연동\n",
    "\n",
    "x = torch.FloatTensor(np.array(x)).to(device)\n",
    "y = torch.LongTensor(np.array(y)).to(device)\n",
    "x_test = torch.FloatTensor(np.array(x_test)).to(device)\n",
    "# 학습 데이터 형태 출력\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(x_test.shape)\n",
    "torch.Size([966, 150])\n",
    "torch.Size([966])\n",
    "torch.Size([322, 150])\n",
    "y.unique()\n",
    "tensor([0, 1, 2, 3, 4, 5, 6])\n",
    "# 랜덤 값 고정\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 총 3개의 레이어 사용\n",
    "\n",
    "linear1 = nn.Linear(150, 450, bias = True)\n",
    "linear2 = nn.Linear(450, 300, bias = True)\n",
    "linear3 = nn.Linear(40, 20, bias = True)\n",
    "linear4 = nn.Linear(300, 7, bias = True)\n",
    "relu = nn.ReLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "# 모델 구성\n",
    "\n",
    "model = nn.Sequential(linear1, relu, linear2, sigmoid, linear4).to(device)\n",
    "\n",
    "# optimizer, loss\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# iteration\n",
    "\n",
    "for stop in range(1500):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(x)\n",
    "    cost = loss(hypothesis, y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if stop % 100 == 0:\n",
    "        print(stop, cost.item())\n",
    "0 2.101914644241333\n",
    "100 0.017238745465874672\n",
    "200 0.0048625594936311245\n",
    "300 0.002484563272446394\n",
    "400 0.00154509034473449\n",
    "500 0.0010670615592971444\n",
    "600 0.0007868750253692269\n",
    "700 0.0006069521186873317\n",
    "800 0.00048376075574196875\n",
    "900 0.0003952813567593694\n",
    "1000 0.00032935081981122494\n",
    "1100 0.00027877147658728063\n",
    "1200 0.00023902812972664833\n",
    "1300 0.00020717622828669846\n",
    "1400 0.0001812183327274397\n",
    "# 결과 확인\n",
    "\n",
    "hypothesis = model(x_test)\n",
    "pred = hypothesis.argmax(dim=1).cpu()\n",
    "result = pred.detach().numpy()\n",
    "submit[\"Category\"] = result\n",
    "submit\n",
    "id\tCategory\n",
    "0\t0\t3\n",
    "1\t1\t3\n",
    "2\t2\t6\n",
    "3\t3\t3\n",
    "4\t4\t3\n",
    "...\t...\t...\n",
    "317\t317\t1\n",
    "318\t318\t3\n",
    "319\t319\t1\n",
    "320\t320\t3\n",
    "321\t321\t2\n",
    "322 rows × 2 columns\n",
    "\n",
    "submit.to_csv(\"q3submit.csv\", index=False)\n",
    "P3 코드 설명\n",
    "Preprocessing\n",
    "데이터 자체가 이미 PCA를 통해 가공이 된 상태라 처음에는 전처리 없이 진행해보았습니다. StandardScaler를 사용했을 때는 성능에 변화가 없었습니다.\n",
    "\n",
    "Training\n",
    "캐글 평가기준이 Categorization Accuracy 였고, 분류할 클래스가 총 7개였기 때문에 Loss Function은 CrossEntropyLoss를 사용하였습니다. 처음에는 레이어를 4개로 구성하였으나, 3개로 줄였을 때 학습이 더 잘 되었습니다. 반복의 경우, 1000으로 두고 했을 때 최고점이 나왔기 때문에 조금 더 늘린 1500으로 시도해도 성능은 같았습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
