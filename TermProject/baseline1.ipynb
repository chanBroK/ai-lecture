{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(1)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv(\"./data/train_data.csv\")\n",
    "test_df = pd.read_csv(\"./data/test_data.csv\")\n",
    "submission_df = pd.read_csv(\"./data/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "area_name = np.array(pd.concat([train_df[\"area_name\"], test_df[\"area_name\"]], axis=0))\n",
    "le.fit(area_name)\n",
    "train_df[\"area_name\"] = le.transform(train_df[\"area_name\"])\n",
    "test_df[\"area_name\"] = le.transform(test_df[\"area_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape : torch.Size([1314, 2]) torch.Size([1314])\n"
     ]
    }
   ],
   "source": [
    "# set data in torch\n",
    "train_x = np.array(train_df.drop([\"gas_usage\", \"year\"], axis=1))\n",
    "test_x = np.array(test_df.drop([\"year\"], axis=1))\n",
    "train_y = np.array(train_df[\"gas_usage\"])\n",
    "\n",
    "train_x = torch.Tensor(train_x).to(device)\n",
    "test_x = torch.Tensor(test_x).to(device)\n",
    "train_y = torch.Tensor(train_y).to(device)\n",
    "\n",
    "print(\"data shape :\", train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and init layer (Use NN)\n",
    "layer1 = torch.nn.Linear(2, 16).to(device)\n",
    "layer2 = torch.nn.Linear(16, 1).to(device)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "torch.nn.init.xavier_normal_(layer1.weight)\n",
    "torch.nn.init.xavier_normal_(layer2.weight)\n",
    "\n",
    "# define model\n",
    "model = torch.nn.Sequential(layer1, relu, layer2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 358.75384521484375\n",
      "100 199.37811279296875\n",
      "200 121.95584106445312\n",
      "300 91.63488006591797\n",
      "400 82.23683166503906\n",
      "500 79.76009368896484\n",
      "600 78.880615234375\n",
      "700 77.50785827636719\n",
      "800 76.72615051269531\n",
      "900 76.03026580810547\n",
      "1000 75.36429595947266\n"
     ]
    }
   ],
   "source": [
    "# set learning param\n",
    "epochs = 2000\n",
    "lr = 1e-4\n",
    "loss = torch.nn.MSELoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# learning\n",
    "for epoch in range(epochs + 1):\n",
    "    output = model(train_x)\n",
    "    cost = loss(output, train_y.unsqueeze(1))\n",
    "\n",
    "    optim.zero_grad()\n",
    "    cost.backward()\n",
    "    optim.step()\n",
    "    if epoch % (epochs / 10) == 0:\n",
    "        print(epoch, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.7245],\n",
      "        [12.4296],\n",
      "        [13.4512],\n",
      "        [11.8854],\n",
      "        [13.6616],\n",
      "        [ 8.8028],\n",
      "        [ 8.8991],\n",
      "        [12.7524],\n",
      "        [12.6161],\n",
      "        [11.9095],\n",
      "        [11.5753],\n",
      "        [12.2359],\n",
      "        [11.8757],\n",
      "        [ 4.3376],\n",
      "        [11.2203],\n",
      "        [ 8.1669],\n",
      "        [ 8.8412],\n",
      "        [ 9.0851],\n",
      "        [ 4.3920],\n",
      "        [ 8.7004],\n",
      "        [11.4731],\n",
      "        [10.7619],\n",
      "        [ 4.8279],\n",
      "        [ 3.0848],\n",
      "        [11.6978],\n",
      "        [10.9154],\n",
      "        [ 5.7104],\n",
      "        [ 9.5915],\n",
      "        [ 2.7892],\n",
      "        [11.3978],\n",
      "        [ 3.8849],\n",
      "        [12.2400],\n",
      "        [ 7.3951],\n",
      "        [ 7.5878],\n",
      "        [ 7.1061],\n",
      "        [12.2270],\n",
      "        [ 3.7601],\n",
      "        [ 5.1836],\n",
      "        [ 6.0265],\n",
      "        [10.9846],\n",
      "        [13.3027],\n",
      "        [ 5.9814],\n",
      "        [13.0829],\n",
      "        [ 9.1092],\n",
      "        [10.6956],\n",
      "        [12.8390],\n",
      "        [11.5867],\n",
      "        [ 3.4566],\n",
      "        [ 8.8653],\n",
      "        [ 9.5674],\n",
      "        [12.1406],\n",
      "        [13.5016],\n",
      "        [ 5.2559],\n",
      "        [11.5213],\n",
      "        [10.9605],\n",
      "        [10.4066],\n",
      "        [ 8.8171],\n",
      "        [14.0222],\n",
      "        [11.6297],\n",
      "        [ 9.6156],\n",
      "        [ 4.2998],\n",
      "        [ 4.4381],\n",
      "        [12.4327],\n",
      "        [ 8.2701],\n",
      "        [ 4.4058],\n",
      "        [13.0788],\n",
      "        [13.4764],\n",
      "        [11.0999],\n",
      "        [ 6.3114],\n",
      "        [13.0347],\n",
      "        [ 5.2384],\n",
      "        [ 4.3459],\n",
      "        [11.8372],\n",
      "        [11.8613],\n",
      "        [ 9.7594],\n",
      "        [ 7.2345],\n",
      "        [12.7186],\n",
      "        [11.0637],\n",
      "        [12.9353],\n",
      "        [14.0770],\n",
      "        [ 5.2077],\n",
      "        [11.3397],\n",
      "        [ 7.5444],\n",
      "        [12.4086],\n",
      "        [12.2882],\n",
      "        [12.7125],\n",
      "        [13.6857],\n",
      "        [ 4.4641],\n",
      "        [ 4.5563],\n",
      "        [ 8.7787],\n",
      "        [10.9636],\n",
      "        [ 4.5102],\n",
      "        [10.9161],\n",
      "        [12.7765],\n",
      "        [12.6402],\n",
      "        [13.4961],\n",
      "        [ 7.0195],\n",
      "        [ 6.8594],\n",
      "        [ 6.1137],\n",
      "        [12.0714],\n",
      "        [12.5609],\n",
      "        [12.0770],\n",
      "        [11.5993],\n",
      "        [11.9418],\n",
      "        [11.9900],\n",
      "        [12.0233],\n",
      "        [12.7365],\n",
      "        [ 6.9085],\n",
      "        [11.2526],\n",
      "        [10.2266],\n",
      "        [ 7.1399],\n",
      "        [ 9.8058],\n",
      "        [10.0262],\n",
      "        [ 5.6026],\n",
      "        [ 5.9932],\n",
      "        [ 5.4997],\n",
      "        [ 6.2769],\n",
      "        [11.1899],\n",
      "        [12.1492],\n",
      "        [11.6475],\n",
      "        [ 8.7630],\n",
      "        [ 2.5660],\n",
      "        [10.1225],\n",
      "        [10.5081],\n",
      "        [10.9402],\n",
      "        [ 3.6277],\n",
      "        [ 5.4515],\n",
      "        [12.5290],\n",
      "        [ 7.2819],\n",
      "        [ 9.3796],\n",
      "        [12.1251],\n",
      "        [ 2.9144],\n",
      "        [12.2641],\n",
      "        [13.5364],\n",
      "        [ 5.2318],\n",
      "        [ 5.6267],\n",
      "        [ 8.4203],\n",
      "        [ 8.4444],\n",
      "        [ 6.2100],\n",
      "        [11.6234],\n",
      "        [12.0022],\n",
      "        [ 7.4811],\n",
      "        [ 6.9326],\n",
      "        [ 8.7148],\n",
      "        [ 4.3597],\n",
      "        [ 7.1640],\n",
      "        [ 6.1618],\n",
      "        [ 7.3059],\n",
      "        [ 8.7389],\n",
      "        [ 9.3556],\n",
      "        [12.1011],\n",
      "        [ 8.0403],\n",
      "        [ 8.0644],\n",
      "        [ 5.4756],\n",
      "        [10.2893],\n",
      "        [ 7.3270],\n",
      "        [14.1667],\n",
      "        [11.4693],\n",
      "        [11.3879],\n",
      "        [11.9299],\n",
      "        [11.9540],\n",
      "        [11.9781],\n",
      "        [11.2767],\n",
      "        [14.0462],\n",
      "        [14.1426],\n",
      "        [13.2515],\n",
      "        [ 6.0896],\n",
      "        [ 6.1377],\n",
      "        [ 6.1859],\n",
      "        [11.1658],\n",
      "        [ 8.1970],\n",
      "        [ 9.4037],\n",
      "        [ 4.7034],\n",
      "        [ 7.0636],\n",
      "        [ 7.4865],\n",
      "        [ 6.2247],\n",
      "        [ 6.2488],\n",
      "        [ 6.2729],\n",
      "        [12.1733],\n",
      "        [ 4.8520],\n",
      "        [ 7.1158],\n",
      "        [ 7.1881],\n",
      "        [ 3.0387],\n",
      "        [10.1784],\n",
      "        [10.2025],\n",
      "        [12.5357],\n",
      "        [11.6716],\n",
      "        [ 7.0436],\n",
      "        [ 7.0677],\n",
      "        [10.9877],\n",
      "        [ 6.0173],\n",
      "        [ 6.4269],\n",
      "        [ 8.4619],\n",
      "        [ 8.4860],\n",
      "        [11.1418],\n",
      "        [ 4.3788],\n",
      "        [ 9.7613],\n",
      "        [13.5604],\n",
      "        [13.7871],\n",
      "        [ 9.3274],\n",
      "        [ 6.6304],\n",
      "        [10.1948],\n",
      "        [ 4.0806],\n",
      "        [ 8.6258],\n",
      "        [ 9.1583],\n",
      "        [ 6.8112],\n",
      "        [ 6.8353],\n",
      "        [ 9.5808],\n",
      "        [11.3399],\n",
      "        [ 6.9463],\n",
      "        [ 6.3454],\n",
      "        [ 5.7180],\n",
      "        [ 5.7662],\n",
      "        [ 6.3695],\n",
      "        [ 5.8016],\n",
      "        [10.9618],\n",
      "        [ 6.6908],\n",
      "        [10.6183],\n",
      "        [13.9758],\n",
      "        [10.4624],\n",
      "        [ 4.2367],\n",
      "        [10.2875],\n",
      "        [ 5.4992],\n",
      "        [ 9.7657],\n",
      "        [ 1.8184],\n",
      "        [ 9.3315],\n",
      "        [ 6.5582],\n",
      "        [10.1466],\n",
      "        [ 2.6888],\n",
      "        [12.3022],\n",
      "        [13.1269],\n",
      "        [ 6.5823],\n",
      "        [ 9.9780],\n",
      "        [ 5.5913],\n",
      "        [ 7.0917],\n",
      "        [13.1692],\n",
      "        [ 4.3547],\n",
      "        [13.6414],\n",
      "        [14.0240],\n",
      "        [13.6896],\n",
      "        [13.7137],\n",
      "        [10.0866],\n",
      "        [10.6969],\n",
      "        [ 8.2886],\n",
      "        [ 7.9578],\n",
      "        [ 8.2645],\n",
      "        [ 2.8163],\n",
      "        [ 3.9437],\n",
      "        [ 5.0406],\n",
      "        [10.9892],\n",
      "        [11.0133],\n",
      "        [ 9.3033],\n",
      "        [ 8.5808],\n",
      "        [ 8.6498],\n",
      "        [ 5.7866],\n",
      "        [ 6.9437],\n",
      "        [ 2.5881],\n",
      "        [ 5.7625],\n",
      "        [ 7.0877],\n",
      "        [11.0373],\n",
      "        [10.2185],\n",
      "        [12.8620],\n",
      "        [ 2.2848],\n",
      "        [ 4.0984],\n",
      "        [ 2.6131],\n",
      "        [ 2.4267],\n",
      "        [13.1933],\n",
      "        [13.3253],\n",
      "        [ 9.6436],\n",
      "        [ 9.6918],\n",
      "        [10.0743],\n",
      "        [ 9.4981],\n",
      "        [10.2188],\n",
      "        [ 9.5703],\n",
      "        [10.2429],\n",
      "        [10.3169],\n",
      "        [ 6.6545],\n",
      "        [13.7378],\n",
      "        [ 4.0522],\n",
      "        [ 4.0345],\n",
      "        [ 5.7384],\n",
      "        [ 3.5484],\n",
      "        [ 3.2533],\n",
      "        [ 2.6486],\n",
      "        [ 3.6102],\n",
      "        [ 6.2479],\n",
      "        [ 9.5954],\n",
      "        [ 2.4433],\n",
      "        [ 8.0317],\n",
      "        [ 8.0558],\n",
      "        [ 9.1342],\n",
      "        [ 9.1823],\n",
      "        [ 9.2064],\n",
      "        [13.2578],\n",
      "        [ 4.7547],\n",
      "        [ 8.4851],\n",
      "        [ 9.6935],\n",
      "        [12.0498],\n",
      "        [ 7.0334],\n",
      "        [ 5.3103],\n",
      "        [11.7055],\n",
      "        [ 5.5232],\n",
      "        [11.6814],\n",
      "        [10.7045],\n",
      "        [ 5.3344],\n",
      "        [ 7.5699],\n",
      "        [10.6042],\n",
      "        [ 5.1620],\n",
      "        [ 7.0401],\n",
      "        [10.7609],\n",
      "        [10.6289],\n",
      "        [ 9.7028],\n",
      "        [11.7296],\n",
      "        [ 6.6480],\n",
      "        [ 6.8010],\n",
      "        [ 5.4784],\n",
      "        [ 9.6212],\n",
      "        [ 8.4369],\n",
      "        [ 7.8040],\n",
      "        [ 7.0160],\n",
      "        [10.4603],\n",
      "        [ 5.5714],\n",
      "        [ 6.6332],\n",
      "        [ 6.9222],\n",
      "        [13.5933],\n",
      "        [ 9.2600],\n",
      "        [ 9.2840],\n",
      "        [ 9.3081],\n",
      "        [ 9.3322],\n",
      "        [10.9453],\n",
      "        [10.9212],\n",
      "        [ 5.5993],\n",
      "        [10.8478],\n",
      "        [ 8.0052],\n",
      "        [10.3410],\n",
      "        [ 8.4625],\n",
      "        [10.6488],\n",
      "        [10.4344],\n",
      "        [ 5.6677],\n",
      "        [ 8.9841],\n",
      "        [ 8.8878],\n",
      "        [ 5.6433],\n",
      "        [13.4990],\n",
      "        [ 5.3585],\n",
      "        [ 9.5490],\n",
      "        [ 9.5730],\n",
      "        [ 9.2118],\n",
      "        [ 9.2359],\n",
      "        [ 5.1514],\n",
      "        [ 6.7993],\n",
      "        [ 8.0799],\n",
      "        [ 7.5235],\n",
      "        [ 7.5717],\n",
      "        [ 5.2862],\n",
      "        [ 5.8107],\n",
      "        [12.7520],\n",
      "        [ 9.6935],\n",
      "        [ 8.4128],\n",
      "        [ 6.9678],\n",
      "        [ 6.9919],\n",
      "        [ 9.2305],\n",
      "        [ 5.5025],\n",
      "        [ 6.7509],\n",
      "        [ 5.1273],\n",
      "        [ 7.4994],\n",
      "        [ 8.2966],\n",
      "        [ 7.3893],\n",
      "        [ 4.8008],\n",
      "        [12.4149],\n",
      "        [ 4.6438],\n",
      "        [11.2589],\n",
      "        [ 8.7060],\n",
      "        [12.3773],\n",
      "        [ 5.0643],\n",
      "        [ 6.6721],\n",
      "        [ 8.4610],\n",
      "        [ 7.7614],\n",
      "        [ 6.7018],\n",
      "        [ 6.7287],\n",
      "        [ 6.8715],\n",
      "        [ 7.2610],\n",
      "        [12.7805],\n",
      "        [ 3.9453],\n",
      "        [ 4.0898],\n",
      "        [ 5.5473],\n",
      "        [ 6.0700],\n",
      "        [10.7663],\n",
      "        [ 8.9119],\n",
      "        [ 2.1589],\n",
      "        [11.8982],\n",
      "        [ 7.0093],\n",
      "        [10.8385],\n",
      "        [ 2.9935],\n",
      "        [ 7.9448],\n",
      "        [ 7.6076],\n",
      "        [10.6048],\n",
      "        [ 3.0222],\n",
      "        [ 3.0827],\n",
      "        [ 3.1104],\n",
      "        [ 5.2956],\n",
      "        [ 5.3197],\n",
      "        [12.5447],\n",
      "        [ 3.2034],\n",
      "        [ 8.7674],\n",
      "        [13.4395],\n",
      "        [ 3.2314],\n",
      "        [ 1.9887],\n",
      "        [11.9223],\n",
      "        [ 5.7773],\n",
      "        [ 7.9207],\n",
      "        [ 3.2918],\n",
      "        [ 6.7528],\n",
      "        [11.9704],\n",
      "        [13.2540],\n",
      "        [ 8.5024],\n",
      "        [11.9463],\n",
      "        [11.8500],\n",
      "        [ 9.7269],\n",
      "        [ 6.0459],\n",
      "        [14.1464],\n",
      "        [13.2781],\n",
      "        [ 7.7630],\n",
      "        [ 7.7871],\n",
      "        [10.2507],\n",
      "        [ 5.7421],\n",
      "        [ 6.8981],\n",
      "        [ 5.1861],\n",
      "        [10.4840],\n",
      "        [ 8.4784],\n",
      "        [ 4.1607],\n",
      "        [12.1872],\n",
      "        [11.8741],\n",
      "        [ 2.5665],\n",
      "        [ 5.8014],\n",
      "        [ 3.0539],\n",
      "        [ 9.7510],\n",
      "        [12.5206],\n",
      "        [12.5687]])\n"
     ]
    }
   ],
   "source": [
    "# submission\n",
    "with torch.no_grad():\n",
    "    predict = model(test_x)\n",
    "    predict = predict.cpu().detach()\n",
    "    submission_df['gas_usage']= predict\n",
    "print(submission_df)\n",
    "submission_df.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
